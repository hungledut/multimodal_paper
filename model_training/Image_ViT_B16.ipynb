{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFnWnDcI3qJJ",
        "outputId": "2d4750b8-90c2-43de-8960-ee0cbda0ddbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OOE3u9uxylz",
        "outputId": "eb222c25-7137-4a16-e8e4-3c503fde6f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available. Using GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"GPU is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSO_G2wVfvmK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "path = \"/content/drive/MyDrive/Data_Image\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "Image_dataset = datasets.ImageFolder(root=path, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsUqpFBGMv_b"
      },
      "outputs": [],
      "source": [
        "# Chia dữ liệu thành tập train và tập test\n",
        "train_dataset, test_dataset = random_split(Image_dataset, [0.7, 0.3])\n",
        "\n",
        "batch_size = 32\n",
        "# Tạo DataLoader để nạp dữ liệu theo batch\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih9LgbcXcrBy",
        "outputId": "73c67a18-8ca9-41ae-f36b-f7d9da222b21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1346"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(Image_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6EGKDgNV-T0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import vision_transformer\n",
        "\n",
        "class VisionTransformerModel(nn.Module):\n",
        "    def __init__(self, num_classes=16):\n",
        "        super(VisionTransformerModel, self).__init__()\n",
        "\n",
        "        self.vit = vision_transformer.vit_b_16(weights=None).to(device)\n",
        "\n",
        "        n_inputs = self.vit.heads[0].in_features\n",
        "        self.vit.heads[0] = nn.Sequential(\n",
        "            nn.Linear(n_inputs, num_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vit(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(Image_dataset.classes)\n",
        "vision_transformer_model = VisionTransformerModel(num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjH2Qxf1FyGN",
        "outputId": "589b1e74-b454-480a-9371-8c8edc0ef81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformerModel(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (heads): Sequential(\n",
            "      (head): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=16, bias=True)\n",
            "        (1): LogSoftmax(dim=1)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(vision_transformer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWQtGz3wceDx"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn):\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Chuyển dữ liệu và mô hình lên GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for (X, y) in data_loader:\n",
        "            # Chuyển dữ liệu lên GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y).item()\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw3F8Rq1K0iG"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(vision_transformer_model.parameters(), lr=0.0001)\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()  # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uVOCDZ3Kv9R",
        "outputId": "b85baadc-d294-4915-8930-4bda2d3f8060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 2.98665 | Train accuracy: 11.60%\n",
            "Test loss: 2.71946 | Test accuracy: 10.11%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 2.67470 | Train accuracy: 14.43%\n",
            "Test loss: 2.65083 | Test accuracy: 15.47%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 2.55144 | Train accuracy: 18.48%\n",
            "Test loss: 2.46888 | Test accuracy: 15.71%\n",
            "\n",
            "Epoch: 3\n",
            "---------\n",
            "Train loss: 2.47683 | Train accuracy: 19.22%\n",
            "Test loss: 2.45511 | Test accuracy: 19.26%\n",
            "\n",
            "Epoch: 4\n",
            "---------\n",
            "Train loss: 2.42880 | Train accuracy: 20.53%\n",
            "Test loss: 2.31390 | Test accuracy: 26.80%\n",
            "\n",
            "Epoch: 5\n",
            "---------\n",
            "Train loss: 2.30177 | Train accuracy: 23.40%\n",
            "Test loss: 2.15339 | Test accuracy: 23.41%\n",
            "\n",
            "Epoch: 6\n",
            "---------\n",
            "Train loss: 2.15155 | Train accuracy: 28.70%\n",
            "Test loss: 2.16862 | Test accuracy: 20.69%\n",
            "\n",
            "Epoch: 7\n",
            "---------\n",
            "Train loss: 2.11554 | Train accuracy: 28.26%\n",
            "Test loss: 2.35116 | Test accuracy: 22.77%\n",
            "\n",
            "Epoch: 8\n",
            "---------\n",
            "Train loss: 1.93808 | Train accuracy: 35.94%\n",
            "Test loss: 1.97895 | Test accuracy: 34.40%\n",
            "\n",
            "Epoch: 9\n",
            "---------\n",
            "Train loss: 1.82893 | Train accuracy: 38.14%\n",
            "Test loss: 2.13325 | Test accuracy: 35.05%\n",
            "\n",
            "Epoch: 10\n",
            "---------\n",
            "Train loss: 1.72984 | Train accuracy: 42.35%\n",
            "Test loss: 2.08495 | Test accuracy: 33.05%\n",
            "\n",
            "Epoch: 11\n",
            "---------\n",
            "Train loss: 1.50033 | Train accuracy: 50.54%\n",
            "Test loss: 2.20818 | Test accuracy: 35.77%\n",
            "\n",
            "Epoch: 12\n",
            "---------\n",
            "Train loss: 1.27919 | Train accuracy: 56.21%\n",
            "Test loss: 2.18513 | Test accuracy: 37.04%\n",
            "\n",
            "Epoch: 13\n",
            "---------\n",
            "Train loss: 1.23637 | Train accuracy: 60.12%\n",
            "Test loss: 2.21831 | Test accuracy: 37.93%\n",
            "\n",
            "Epoch: 14\n",
            "---------\n",
            "Train loss: 0.99422 | Train accuracy: 68.40%\n",
            "Test loss: 2.25738 | Test accuracy: 38.56%\n",
            "\n",
            "Epoch: 15\n",
            "---------\n",
            "Train loss: 0.82581 | Train accuracy: 71.99%\n",
            "Test loss: 2.41858 | Test accuracy: 39.30%\n",
            "\n",
            "Epoch: 16\n",
            "---------\n",
            "Train loss: 0.70981 | Train accuracy: 75.57%\n",
            "Test loss: 2.48653 | Test accuracy: 36.64%\n",
            "\n",
            "Epoch: 17\n",
            "---------\n",
            "Train loss: 0.57560 | Train accuracy: 81.31%\n",
            "Test loss: 2.51541 | Test accuracy: 40.66%\n",
            "\n",
            "Epoch: 18\n",
            "---------\n",
            "Train loss: 0.47316 | Train accuracy: 83.80%\n",
            "Test loss: 2.72351 | Test accuracy: 37.53%\n",
            "\n",
            "Epoch: 19\n",
            "---------\n",
            "Train loss: 0.37083 | Train accuracy: 88.62%\n",
            "Test loss: 3.11155 | Test accuracy: 33.60%\n",
            "\n",
            "Epoch: 20\n",
            "---------\n",
            "Train loss: 0.30818 | Train accuracy: 89.88%\n",
            "Test loss: 2.85128 | Test accuracy: 39.31%\n",
            "\n",
            "Epoch: 21\n",
            "---------\n",
            "Train loss: 0.26577 | Train accuracy: 91.25%\n",
            "Test loss: 3.41596 | Test accuracy: 37.92%\n",
            "\n",
            "Epoch: 22\n",
            "---------\n",
            "Train loss: 0.32845 | Train accuracy: 89.99%\n",
            "Test loss: 2.86426 | Test accuracy: 38.56%\n",
            "\n",
            "Epoch: 23\n",
            "---------\n",
            "Train loss: 0.29429 | Train accuracy: 90.49%\n",
            "Test loss: 3.13057 | Test accuracy: 40.17%\n",
            "\n",
            "Epoch: 24\n",
            "---------\n",
            "Train loss: 0.19958 | Train accuracy: 93.20%\n",
            "Test loss: 3.31158 | Test accuracy: 38.66%\n",
            "\n",
            "Epoch: 25\n",
            "---------\n",
            "Train loss: 0.12492 | Train accuracy: 95.60%\n",
            "Test loss: 3.42033 | Test accuracy: 40.03%\n",
            "\n",
            "Epoch: 26\n",
            "---------\n",
            "Train loss: 0.13020 | Train accuracy: 95.72%\n",
            "Test loss: 3.60190 | Test accuracy: 39.04%\n",
            "\n",
            "Epoch: 27\n",
            "---------\n",
            "Train loss: 0.19906 | Train accuracy: 94.33%\n",
            "Test loss: 3.40658 | Test accuracy: 41.22%\n",
            "\n",
            "Epoch: 28\n",
            "---------\n",
            "Train loss: 0.18069 | Train accuracy: 93.74%\n",
            "Test loss: 3.29131 | Test accuracy: 39.52%\n",
            "\n",
            "Epoch: 29\n",
            "---------\n",
            "Train loss: 0.16202 | Train accuracy: 94.67%\n",
            "Test loss: 3.40812 | Test accuracy: 40.03%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    vision_transformer_model.train()\n",
        "\n",
        "    train_step(data_loader=train_loader,\n",
        "        model=vision_transformer_model,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "    vision_transformer_model.eval()\n",
        "    test_step(data_loader=test_loader,\n",
        "        model=vision_transformer_model,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}