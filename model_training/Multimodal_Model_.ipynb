{"cells":[{"cell_type":"markdown","metadata":{"id":"QSlKQ7bo5lAy"},"source":["## Data Preparation For Description"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qB-gMbdR5Q3y","colab":{"base_uri":"https://localhost:8080/","height":423},"outputId":"322ccf57-943e-4dc8-e054-89136698b1cd","executionInfo":{"status":"ok","timestamp":1710674662041,"user_tz":-420,"elapsed":428,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          Description  Label\n","0   [  Item Dimensions : 13.4L x 11.5W x 22.8H inc...  table\n","1   [  Simple stylish design with ample storage an...  table\n","2   [  The minimalist design desk serves good purp...  table\n","3   [  Card table with vinyl playing surface mater...  table\n","4   [  Simple stylish design, Functional and suita...  table\n","..                                                ...    ...\n","65                                                 []   iron\n","66  [  MULTI DIRECTIONAL IRONING – The streamlined...   iron\n","67  [  THICKER STAINLESS STEEL TANK - All new stai...   iron\n","68  [  MORE POWER: Classic Steam Iron comes equipp...   iron\n","69  [  Dynamic Steam Technology - The Allure iron ...   iron\n","\n","[1439 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-3f75322d-a0da-4f4f-9167-8185dc819625\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Description</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[  Item Dimensions : 13.4L x 11.5W x 22.8H inc...</td>\n","      <td>table</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[  Simple stylish design with ample storage an...</td>\n","      <td>table</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[  The minimalist design desk serves good purp...</td>\n","      <td>table</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[  Card table with vinyl playing surface mater...</td>\n","      <td>table</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[  Simple stylish design, Functional and suita...</td>\n","      <td>table</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>[]</td>\n","      <td>iron</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>[  MULTI DIRECTIONAL IRONING – The streamlined...</td>\n","      <td>iron</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>[  THICKER STAINLESS STEEL TANK - All new stai...</td>\n","      <td>iron</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>[  MORE POWER: Classic Steam Iron comes equipp...</td>\n","      <td>iron</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>[  Dynamic Steam Technology - The Allure iron ...</td>\n","      <td>iron</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1439 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f75322d-a0da-4f4f-9167-8185dc819625')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3f75322d-a0da-4f4f-9167-8185dc819625 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3f75322d-a0da-4f4f-9167-8185dc819625');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7bc3c470-6d47-4a37-afdb-fc0abd9b6e1c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7bc3c470-6d47-4a37-afdb-fc0abd9b6e1c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7bc3c470-6d47-4a37-afdb-fc0abd9b6e1c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_4e6fa90d-e6e6-4776-87b6-b4d04e8ab724\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_4e6fa90d-e6e6-4776-87b6-b4d04e8ab724 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 1439,\n  \"fields\": [\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1284,\n        \"samples\": [\n          \"[  TrueGlide Soleplate: The nonstick soleplate provides a 2x smoother glide on all fabric types   ,   Cord Reel: The spring-loaded cord unreels and retracts quickly for easy use and storage   ,   Fabric Guide: Find the perfect temperature for all fabric types with the convenient guide printed on the top of the iron   ,   Anti-Drip: This feature eliminates pesky water spotting by properly heating water before it enters the steam chamber   ]\",\n          \"[  HERE COMES THE BASS: Pronounced beats. Stronger vibes. These wireless on-ear headphones boast a BASS boost button for deeper bass at a touch. Powerful 32mm drivers bring the best out of your favorite soundtracks.   ,   ANYTIME, ANYWHERE: You get up to 29 hours playtime from a 2-hour charge via USB-C, and if you need a little extra juice, a 15-minute boost will give you another 4 hours.   ,   DESIGNED FOR LONG-WEARING COMFORT: These on-ear headphones boast a cushioned headband so light you can barely feel it. The soft ear cups are clearly marked for left/right ears, and can be angled until they feel just right. The matte color is smart yet understated - the perfect finishing touch to your everyday look.   ,   AT YOUR FINGERTIPS: You can take calls, pause music, and wake your voice assistant with a press of the multifunction button. These headphones are ready to pair as soon as you turn them on.   ,   STAY ON-THE-MOVE: The ear cups on these headphones fold flat and swivel inward, so they can easily fit in your pocket or handbag.   ,   INCLUDED ACCESSORIES: USB-C cable, quick start guide.   ]\",\n          \"[  SMART TV EXPERIENCE: Enjoy Netflix, YouTube and other streaming services by simply connecting the monitor to WiFi; Samsung TV Plus also offers free live and on-demand content with no downloads or sign-up needed, while Universal Guide provides personalized content recommendations.Viewing Angle:178.0 degrees.Response time:4.0 milliseconds   ,   ICONIC SLIM DESIGN: The Smart Monitor is built with your lifestyle in mind, delivering a more cutting-edge design than ever; With an ultra-slim flat back, neat camera design and beautiful colors, the monitor offers a minimalist look that suits any environment and enhances your setup with super style   ,   PC-LESS PRODUCTIVITY: Browse the web, edit documents and work on projects, all without the need for a separate PC; With new Workmode, you can also remotely access another PC, use Microsoft Office 365 programs and even connect to Samsung mobile devices with Samsung DeX for seamless working   ,   IoT HUB: Turn your Smart Monitor into a control center for the home by connecting it to your IoT home products with SmartThings; Relax by turning off the lights, pulling down the blinds and setting the thermostat to the perfect temperature all with the Smart Monitor for a perfectly relaxed evening   ]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"table\",\n          \"cell+phone\",\n          \"chair\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","import torch\n","\n","products = ['table','cell+phone','laptop','keyboard','headphones','chair',\n","            'lamp','fridge','mouse','printer','tablet','television',\n","            'vacuum','digital+camera','speaker','iron']\n","\n","df = pd.DataFrame()\n","for product in products:\n","  df_product = pd.read_csv(product+'_Text.csv')\n","  df_product['Label'] = df_product.shape[0] * [product]\n","  df = pd.concat([df,df_product])\n","\n","df.columns = ['Description', 'Label']\n","df"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eb098sg5iFx","outputId":"a4a380a8-e5f3-492a-91a5-bc3a424dcf42","executionInfo":{"status":"ok","timestamp":1710674744976,"user_tz":-420,"elapsed":78713,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m153.6/156.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.5.1\n"]}],"source":["!pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545,"referenced_widgets":["d2d5a20ecd96444cbdd51e467427eb5e","8bdeb63e5e034d3183850a9bb3fd1a8d","ae1e3aef51384973ad65ff83778b0431","20458aea57474325a397ad2ef4417b33","50a3fcf7824f499da72067ab384337de","0a541f9de0774eabbdc55abfcd145ff5","b74c23e378214db9977a3e29e5734685","0f76c850bb6b4456b343b0842aa7560a","b7b121821779430cbb3009202823897a","d14f470cd96545819c473bdbe047e69e","934483888dd24eea9c6c2ac8efece25f","58603e9f5f5845dfba2ac318a6cae79d","65453e380ae9437ab79ed568b5038b8a","5770ebda679942f8aaedfbe93d1c2cef","601f83a08fe041c3be23403e9d4af97e","753279af87334d27a4537d35259e999c","e194d18077bf48f09f64bed12aa12eda","7e8a0cd2b13a4d32be15424a195005a9","f00b2874750b44609e26e2e8888a8e1d","b6c035b4a50c49d69b5b230e0113ec4c","acc6005120864598b2ecdf7c224b1012","b70e1353f8fc4b42ad3dc468f1b85c14","e322bef520b342cab52efe6005e1ff60","8ba254b4f4d547b79784482e3046bb80","5a69d0a80dd648f9adabfdabf002164c","538e252ce2ba4c58886eb2a5ebbac01a","c8e57c101ae0486aab77f3ddd7ccf7f6","a47e8a22cb2641cebc2c5a6f9395559b","b365806041de4e9183fa9508277d412e","bf91bd8c5dea492c8ff9753399d118d4","d4154870c72a435bb0c5223911a97dbb","4b83483eac964603a5b9f016b3ff3814","368c91f4a18a4335a871e59a7221fa3c","37376a97cfad4f0b8bea034317763910","fb2ce4eb2dc240e1af5c863623a2db51","33502b74c85243518773408fb92ca2fb","f922cedae89d4df58a294cdb95b1a2fa","e9210da4c54d4dc6a461453f14a98370","406cd0c77f3c4a909aee38ddf48b157d","ff72018a7e60406dad78c79b6da61ef7","24ad674dd1d74903a681c9c3345afbb1","13eb07ddfc904bb2a711991edfb6f9cb","ee59ad0c5b014add866eb43e6a150f6c","8ac043b94e96435aacf6e2c552e83e47","07604382e84e4e288689bb587424b551","dd04c2f452dc4669a74d0d206fc97ac0","5475a7583f384a2892a9c775123f8e2d","b88533021cdb421eac692704e66b54bd","60872dcbb5c749d09496669a5c192fe6","9fd4e3e657c547f18a44077b19fb4f1a","048900ed59f64799a029c0a32cc9dd06","eb6f94dcad844c1f88f2a417947777db","c41fb20eb1c44417aaa01158e02f03c1","ccf8bc1082ed419fbe846edf200d04fe","25a01e32bacd49dd8988be06b476120b","7b1de8510c424a7caed91f78f887e119","a7d8b029ad944ef49017682774b2085d","60b6e03163544762b382db0f4ee200d1","5918bce8f41c4b919a241fd3e01f98fb","e6644e8416c3490ea4585edc5394ab2d","87567b4ab0d9461ab043f05d180da961","de86f64545e1428281605e3c2a91aa48","112d74b8db6944b49f78c777316bbbba","e7aa45a5bff14baab9634b46edc59582","0ae2afb2ccaa41c89966c9880d230735","4405a044b7f847769b38bd7c14f95aa9","a7e86bbd33764242a8b5d8bb059a1fa9","c8a5adadcd704e389316276a739c80a2","14238288a11b4e67b15c55aee4fcfcda","3252e7b52987426e86023683341c1ba3","321396c4e1ad418fb8e9239a0f877420","a81108fccefa4cbc808dd3f2c4c5c6d8","178fe89a113f4126b5367af73899f1a2","3cedcd5bbd614f58b506c995f0c688c2","2cea04f3cce0457da2a57ac8097a3c5a","76e47fbdf99649848e4dd639bcecf30f","42748b121ee146e3b3f9ebaed1770887","e605076008394530aa6a8a187b121bdf","accd738877134610b5c8c2c7b07a975e","90959cfb066543679518e8af3ca9dbf7","92940d4511c84a7384e832d23322ebd4","e1722be15c8745588bd65b09e837552d","5332efeb6de64994b6977f63517e9bb6","34c6e06c39bf4e79858b9a1408b5999f","ac0f70c56da747dd902a0a1e8d9e6914","248b6c0d2fae4774813d926ce0e00d35","982ab752617545b197b32001b3676629","e795a2719dcf49afa32fb33d718d65d4","abfb9cb3931a4352b2a5112b371dd06d","7b6414efeb874a2f9186601a1b7b7349","33a920eea663491f8f3fec25da6bd749","736c7783cb0845f7b2a84ab870cfd41c","894b491cb3cc4a8ab0bf4d895dd5b271","6bfa8371f8dd4b07a1cb02af4b6b3082","600099c19a824a2092be84e726fc0b63","726135d999f74ec49e8990a06e0ef970","dffacd93f675470e9734fa6ee60222f4","1d93a4fd4d2c4244b0db562a74a4c5f0","49799bf49a1548dfb2d7ca1661e9eca5","bfd69952aa9f48358fcdb99490595187","1f82dc95ed7446e49dcd5a06ce6e436d","c057ccc84087452497dc56c1eaa93627","b4e1cf8299064cdd8cab8672dd6e31b0","a0c8e3e0a7aa404fbdbea51cf28cf655","b5d5cd10ae7e458b8ca4b9e019dd0790","2a06cb3b687e4142b5779b96354ecce0","6d6cbd5173d14b0e80fd7a927b2c8d80","a322ff398e3e4b008e68360dda8542c0","d46ff0bd8af3440ea8b929be66fb2ea0","ebafa061fc7a4d20b9dcc2a521b5d620","aebc0b6e56e64a268414cd3253b802b9","21572f8f2c114e4ea4f3f11fa62ace23","675d52bb107849b186ddc604a8fb9158","55a1294713444227af596fa400b76425","40be60d8de33473ba74176ad942b5a51","70fbf87d0a654c2796c23a1f53a3dd93","7c06d0d1e2734b83b15593cb0be7600c","3af87eb99e854724a41e0c44d2f7bddb","b4b5cd0237b0459eabdb8c0a1298c053","114b7eeae87c4bad8fe2b7fcedad4a5a","6352c68002b14cf895fb2c5097adba49"]},"id":"DyVjW27GCPos","outputId":"c5231bcb-b9ae-4d4d-fa07-328d2b6aca11","executionInfo":{"status":"ok","timestamp":1710674799083,"user_tz":-420,"elapsed":54113,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2d5a20ecd96444cbdd51e467427eb5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58603e9f5f5845dfba2ac318a6cae79d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e322bef520b342cab52efe6005e1ff60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37376a97cfad4f0b8bea034317763910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07604382e84e4e288689bb587424b551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1de8510c424a7caed91f78f887e119"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e86bbd33764242a8b5d8bb059a1fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e605076008394530aa6a8a187b121bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abfb9cb3931a4352b2a5112b371dd06d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd69952aa9f48358fcdb99490595187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aebc0b6e56e64a268414cd3253b802b9"}},"metadata":{}}],"source":["from sentence_transformers import SentenceTransformer\n","tokenizer = SentenceTransformer('sentence-transformers/use-cmlm-multilingual')\n","#embeddings = model.encode(sentences)"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer\n","\n","text = tokenizer.encode(df['Description'].tolist())\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","Labels_text = label_encoder.fit_transform(df['Label'].tolist())\n"],"metadata":{"id":"eXFhii7DVUN7","executionInfo":{"status":"ok","timestamp":1710674818311,"user_tz":-420,"elapsed":19240,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r08z8tJArqqb"},"source":["## Data Preparation For Image"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"HVtvaJUifvlY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f844675-24f2-41cd-8628-419a5cff402b","executionInfo":{"status":"ok","timestamp":1710674845739,"user_tz":-420,"elapsed":27440,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","from torch import nn\n","import torch\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KSO_G2wVfvmK","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"error","timestamp":1710674846229,"user_tz":-420,"elapsed":494,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}},"outputId":"8702dca9-fcb2-4868-b146-9605e4321ba4"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './drive/MyDrive/VietHung_PBL6_demo/Data_Image'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f9b35d971d70>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mImage_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDrive/VietHung_PBL6_demo/Data_Image'"]}],"source":["import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","\n","\n","path = \"./drive/MyDrive/VietHung_PBL6_demo/Data_Image\"\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","Image_dataset = datasets.ImageFolder(root=path, transform=transform)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQPCmTXFvBPg","executionInfo":{"status":"aborted","timestamp":1710674846230,"user_tz":-420,"elapsed":4,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","for (X1, y1) in (Image_dataset):\n","  plt.imshow(X1.permute(1, 2, 0))\n","  print(X1.shape)\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quwb6JxU5iPZ","executionInfo":{"status":"aborted","timestamp":1710674846230,"user_tz":-420,"elapsed":3,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"outputs":[],"source":["image = []\n","Labels_image = []\n","for (X, y) in Image_dataset:\n","  image.append(X)\n","  Labels_image.append(y)"]},{"cell_type":"code","source":["import numpy as np\n","Data_Fusion = []\n","#Image\n","image = np.array(image)\n","Labels_image = np.array(Labels_image)\n","#Text\n","text = np.array(text)\n","Labels_text = np.array(Labels_text)\n","\n","\n","for i in range(len(products)):\n","  for X1,y1,X2,y2 in zip(image[Labels_image==i],Labels_image[Labels_image==i],text[Labels_text==i],Labels_text[Labels_text==i]):\n","    Data_Fusion.append((X1,X2,i))\n","    print(X1.shape,X2.shape,i)\n"],"metadata":{"id":"iNmT6VgaZ36s","executionInfo":{"status":"aborted","timestamp":1710674846230,"user_tz":-420,"elapsed":3,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(Data_Fusion)"],"metadata":{"id":"OwrcK9NRkCMt","executionInfo":{"status":"aborted","timestamp":1710674846230,"user_tz":-420,"elapsed":3,"user":{"displayName":"Việt Hưng Lê","userId":"15174182027916665109"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsUqpFBGMv_b"},"outputs":[],"source":["train_dataset, test_dataset = random_split(Data_Fusion, [0.7, 0.3])\n","\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exhDGtZ5i6M_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"004a8389-d200-4ede-94ce-d09cd2f94ab7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 68.2MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=512, out_features=1000, bias=True)"]},"metadata":{},"execution_count":21}],"source":["from torchvision.models import resnet18,vgg16\n","\n","model = resnet18(pretrained=True)\n","\n","# for param in model.parameters():\n","# \tparam.requires_grad = False\n","\n","# modelOutputFeats = model.fc.in_features\n","model.fc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6EGKDgNV-T0"},"outputs":[],"source":["import torch.nn as nn\n","\n","class Vision_model(nn.Module):\n","    def __init__(self, output_features=128):\n","        super(Vision_model, self).__init__()\n","\n","        self.resnet18 = resnet18(pretrained=False)\n","\n","        modelOutputFeats = self.resnet18.fc.in_features\n","\n","        self.resnet18.fc = nn.Linear(modelOutputFeats,output_features)\n","    def forward(self, x):\n","\n","        x = self.resnet18(x)\n","        return x\n"]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class Language_model(nn.Module):\n","    def __init__(self, input_size=768, output_features=128):\n","        super(Language_model, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_size, 256,bias=True),\n","            nn.ReLU(),\n","            nn.Linear(256, output_features,bias=True),\n","        )\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x"],"metadata":{"id":"FnFMC_EQnon3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Multimodal(nn.Module):\n","    def __init__(self, Vision_model,Language_model,num_classes,output_features=128):\n","        super(Multimodal , self).__init__()\n","        self.vision_model = Vision_model()\n","        self.language_model = Language_model()\n","\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8)\n","\n","        self.fc1 = nn.Linear(output_features*2, 128,bias=True)\n","        self.fc2 = nn.Linear(128, num_classes,bias=True)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","    def forward(self, X1, X2):\n","        X1 = self.vision_model(X1)\n","        X2 = self.language_model(X2)\n","\n","        x = torch.cat([X1,X2],dim=1)\n","        # transformer_encoder = nn.TransformerEncoder(self.encoder_layer,num_layers=1)\n","        # x = transformer_encoder(x)\n","\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"wh48oNJUdrg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = len(Image_dataset.classes)\n","multimodal = Multimodal(Vision_model,Language_model,num_classes)"],"metadata":{"id":"srP8T6_rhIP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWQtGz3wceDx"},"outputs":[],"source":["def train_step(model: torch.nn.Module,\n","               data_loader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               accuracy_fn):\n","    train_loss, train_acc = 0, 0\n","\n","    for batch, (X1, X2, y) in enumerate(data_loader):\n","        print(X1.shape, X2.shape, y.shape)\n","        # 1. Forward pass\n","        y_pred = model(X1,X2)\n","\n","        # 2. Calculate loss\n","        loss = loss_fn(y_pred, y)\n","        print(loss,y.shape,y_pred.shape)\n","        train_loss += loss\n","        train_acc += accuracy_fn(y_true=y,\n","                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","    # Calculate loss and accuracy per epoch and print out what's happening\n","    train_loss /= len(data_loader)\n","    train_acc /= len(data_loader)\n","    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n","\n","def test_step(data_loader: torch.utils.data.DataLoader,\n","              model: torch.nn.Module,\n","              loss_fn: torch.nn.Module,\n","              accuracy_fn):\n","    test_loss, test_acc = 0, 0\n","    model.eval() # put model in eval mode\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        for (X1, X2, y) in data_loader:\n","            # 1. Forward pass\n","            test_pred = model(X1,X2)\n","\n","            # 2. Calculate loss and accuracy\n","            test_loss += loss_fn(test_pred, y)\n","            test_acc += accuracy_fn(y_true=y,\n","                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n","            )\n","\n","        # Adjust metrics and print out\n","        test_loss /= len(data_loader)\n","        test_acc /= len(data_loader)\n","        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bw3F8Rq1K0iG"},"outputs":[],"source":["import torch.optim as optim\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(multimodal.parameters(), lr=0.0001)\n","# Calculate accuracy (a classification metric)\n","def accuracy_fn(y_true, y_pred):\n","    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n","    acc = (correct / len(y_pred)) * 100\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uVOCDZ3Kv9R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"adf8a8f2-705a-443d-f3cf-c52244434e6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7543, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7399, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7024, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7439, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7022, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7357, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6400, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7285, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6803, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5740, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6241, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5979, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5613, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5249, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6324, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6554, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6363, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5411, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5318, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5454, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5589, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4049, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4168, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5499, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5855, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3970, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5337, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3792, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5623, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(2.3249, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 2.58549 | Train accuracy: 18.30%\n","Test loss: 2.49171 | Test accuracy: 19.00%\n","\n","Epoch: 1\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6249, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6736, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4674, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9302, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(3.2237, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.9412, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.7235, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5581, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4752, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5437, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6361, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5215, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4975, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5300, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4180, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5809, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5089, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3008, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6202, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6556, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2499, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4183, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3965, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1816, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2302, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3700, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5454, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5603, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1703, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(2.3829, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 2.49788 | Train accuracy: 19.01%\n","Test loss: 2.49030 | Test accuracy: 18.19%\n","\n","Epoch: 2\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2047, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.5873, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3425, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3364, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1255, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2512, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4809, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2822, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0201, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2940, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2284, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2769, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1462, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2146, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4279, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3599, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1863, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2693, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3481, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1580, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6399, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3240, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2128, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4408, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0923, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2430, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.6375, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3685, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3403, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(2.0933, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 2.29776 | Train accuracy: 22.35%\n","Test loss: 2.27308 | Test accuracy: 27.91%\n","\n","Epoch: 3\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4121, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4703, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2604, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2189, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1723, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1764, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3168, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2270, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2075, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1585, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8556, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1017, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2341, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1785, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0712, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2716, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7831, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1998, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1610, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0591, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9850, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0438, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1229, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2181, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9753, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9164, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0436, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7092, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0462, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(2.0742, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 2.12236 | Train accuracy: 28.51%\n","Test loss: 2.06501 | Test accuracy: 31.60%\n","\n","Epoch: 4\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1471, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8380, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8833, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1226, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8748, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0615, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9712, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9919, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7266, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2183, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8448, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8371, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7510, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.4187, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7944, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7550, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8440, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0390, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9869, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0032, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7585, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0595, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9947, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0504, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8664, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8194, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7619, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8344, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.3751, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(2.1530, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.95942 | Train accuracy: 34.72%\n","Test loss: 1.98308 | Test accuracy: 35.36%\n","\n","Epoch: 5\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6795, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5119, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0675, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9412, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8570, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8109, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1555, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8717, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9200, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7563, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6598, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0735, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6806, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4265, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0302, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5627, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6720, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8900, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8790, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2049, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4252, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1956, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6125, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7209, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6386, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8011, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6248, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8883, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7778, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(1.6728, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.80028 | Train accuracy: 41.24%\n","Test loss: 1.84868 | Test accuracy: 40.83%\n","\n","Epoch: 6\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6590, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7963, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9598, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8896, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7418, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5280, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6066, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7425, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6417, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5211, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.1313, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8234, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7815, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.0460, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3657, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9478, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6708, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7372, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8489, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2942, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6672, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8257, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7545, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5841, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.9145, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5797, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5605, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7630, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5653, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(1.2726, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.74068 | Train accuracy: 41.27%\n","Test loss: 1.72601 | Test accuracy: 43.46%\n","\n","Epoch: 7\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5142, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4486, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7581, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.8579, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4205, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3988, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3885, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(2.2585, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5170, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4359, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4811, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7380, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5260, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2153, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1119, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6672, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3912, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3286, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5526, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3577, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6112, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7058, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1730, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4942, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3761, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3876, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5208, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5449, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3678, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(1.2976, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.49488 | Train accuracy: 51.39%\n","Test loss: 1.54832 | Test accuracy: 53.43%\n","\n","Epoch: 8\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1917, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6092, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4244, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0678, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6871, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7635, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1681, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3325, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1640, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2748, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6526, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2588, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3880, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2356, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6654, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.6526, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.7810, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1202, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4825, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4023, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4138, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0870, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5706, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2306, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3021, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1855, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2598, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9694, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4640, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(1.6574, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.38208 | Train accuracy: 58.16%\n","Test loss: 1.43751 | Test accuracy: 54.76%\n","\n","Epoch: 9\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1382, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1627, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3232, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9505, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4848, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1941, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1746, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0108, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9928, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2919, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3966, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1530, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4535, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0290, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0933, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0838, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4549, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1385, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0864, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9933, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0209, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0201, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1824, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2217, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8997, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2391, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1796, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1411, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2366, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(1.4722, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.17397 | Train accuracy: 65.74%\n","Test loss: 1.36738 | Test accuracy: 61.27%\n","\n","Epoch: 10\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3106, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2199, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2487, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2308, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1183, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9661, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0444, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7198, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9433, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2519, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4131, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8978, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.5347, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0384, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7812, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0236, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1118, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3160, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8110, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3895, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9242, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3642, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0588, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.3599, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1119, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8551, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2968, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7602, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8202, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.9229, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 1.09484 | Train accuracy: 65.70%\n","Test loss: 1.37033 | Test accuracy: 56.36%\n","\n","Epoch: 11\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1506, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0602, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9478, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7921, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8607, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1364, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8001, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0749, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7925, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6935, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1461, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2536, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2183, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7181, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8539, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9956, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9543, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.1283, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7359, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0367, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8197, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8203, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4429, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5990, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2186, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7310, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9793, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6018, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5076, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.6258, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.88985 | Train accuracy: 71.61%\n","Test loss: 1.21877 | Test accuracy: 64.01%\n","\n","Epoch: 12\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6684, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.4022, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8806, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8396, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8731, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2401, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9094, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6892, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8919, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9203, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0065, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8652, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8022, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7523, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8119, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9594, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9339, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2056, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0655, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0229, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0121, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0609, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7254, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8342, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5467, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7046, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7227, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9254, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6875, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.3394, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.87664 | Train accuracy: 72.26%\n","Test loss: 1.05187 | Test accuracy: 66.38%\n","\n","Epoch: 13\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9816, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8773, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6842, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9404, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5099, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7307, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4011, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7011, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6069, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7199, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6856, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6032, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5778, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9519, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6005, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5746, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4942, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6329, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7040, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8220, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6258, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7654, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5155, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5776, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5385, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8084, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8628, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8133, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(1.5675, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.72423 | Train accuracy: 76.26%\n","Test loss: 1.17476 | Test accuracy: 64.79%\n","\n","Epoch: 14\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4733, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6560, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8514, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8143, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5569, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5699, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7245, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7136, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5528, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.2140, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9074, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4355, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6877, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4783, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8712, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7465, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7315, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9639, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6016, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5029, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7782, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7812, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4321, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(1.0285, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9048, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6382, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5502, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4472, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9962, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.6725, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.70942 | Train accuracy: 76.55%\n","Test loss: 0.95493 | Test accuracy: 68.08%\n","\n","Epoch: 15\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5548, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6215, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5310, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6791, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7535, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7772, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4473, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8075, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5846, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7160, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8284, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7435, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6844, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6117, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6553, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6699, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5383, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6774, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4925, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5216, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5525, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6521, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6309, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8999, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3970, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6628, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5214, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5433, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3240, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.7321, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.62705 | Train accuracy: 80.19%\n","Test loss: 0.90012 | Test accuracy: 70.81%\n","\n","Epoch: 16\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6697, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4494, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6773, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4883, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3065, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5267, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3671, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2499, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8001, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6008, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6042, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6907, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7657, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6020, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6057, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.9410, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4453, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4376, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7245, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4071, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3948, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4773, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4545, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8475, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7347, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4756, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5691, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4127, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4949, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.4877, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.55695 | Train accuracy: 81.01%\n","Test loss: 0.94544 | Test accuracy: 71.22%\n","\n","Epoch: 17\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6196, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3330, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4776, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2241, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4362, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4212, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4650, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5705, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3600, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6147, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3019, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3611, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5655, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5790, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7021, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4564, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4330, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5422, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4799, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3083, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4025, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4389, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5709, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7357, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5201, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7500, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4266, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6916, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5432, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.3356, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.48888 | Train accuracy: 83.32%\n","Test loss: 0.92267 | Test accuracy: 73.37%\n","\n","Epoch: 18\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5363, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4823, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3993, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8850, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5034, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4902, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3258, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5186, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5599, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5090, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4960, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7183, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6318, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3809, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5979, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3044, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3662, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5426, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2754, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4550, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4566, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4197, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3039, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4453, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3909, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4856, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5972, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3989, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5029, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.3262, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.47684 | Train accuracy: 83.72%\n","Test loss: 0.88383 | Test accuracy: 71.94%\n","\n","Epoch: 19\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3355, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3773, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2145, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2373, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2191, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4432, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2336, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2690, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4059, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2546, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3035, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4134, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4505, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5077, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5050, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2398, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3047, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3646, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3456, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4336, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3231, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5007, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2294, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1865, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3075, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3893, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3086, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5627, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2682, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.3702, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.34349 | Train accuracy: 89.88%\n","Test loss: 1.01606 | Test accuracy: 71.28%\n","\n","Epoch: 20\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5300, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5292, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3554, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5341, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4771, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2794, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2938, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3988, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3039, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2608, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6331, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2629, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4345, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5810, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2532, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5650, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5123, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2659, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3302, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5735, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3103, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3034, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3352, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4527, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2934, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2167, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2882, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2518, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4903, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.0914, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.38025 | Train accuracy: 87.40%\n","Test loss: 0.82613 | Test accuracy: 76.19%\n","\n","Epoch: 21\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1239, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3986, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2238, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2869, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2256, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1585, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2496, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1942, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2723, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1705, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2259, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1784, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3421, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2146, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2914, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2752, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1904, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2174, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2927, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3447, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2284, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2828, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2958, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3943, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2549, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2104, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1989, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.6695, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3324, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.3730, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.27056 | Train accuracy: 92.68%\n","Test loss: 0.93505 | Test accuracy: 74.66%\n","\n","Epoch: 22\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3585, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1566, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2960, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4159, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3519, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4276, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3165, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3379, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2761, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4809, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0883, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2693, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2570, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5270, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3733, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3849, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3789, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3854, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3938, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2520, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2342, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8443, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.8771, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2277, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2862, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5278, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4242, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4466, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3977, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.7150, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.39029 | Train accuracy: 88.19%\n","Test loss: 0.85414 | Test accuracy: 74.73%\n","\n","Epoch: 23\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3934, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3903, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3543, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3889, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.7325, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1846, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4541, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3199, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4409, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2667, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3049, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3950, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1420, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2295, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3960, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5163, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3793, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1159, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1338, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1654, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1886, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2429, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2802, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4404, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2018, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5060, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2117, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2052, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3680, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.2683, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.32055 | Train accuracy: 90.39%\n","Test loss: 0.89379 | Test accuracy: 74.09%\n","\n","Epoch: 24\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2255, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2953, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2561, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1570, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3780, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2000, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3058, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0991, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3210, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3309, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1420, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1576, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1307, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1572, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1883, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1282, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2682, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1344, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2352, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2560, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1895, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1436, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1104, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3304, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2531, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2439, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2417, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2149, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1390, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.0868, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.21065 | Train accuracy: 93.54%\n","Test loss: 0.90083 | Test accuracy: 75.86%\n","\n","Epoch: 25\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1414, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4180, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1821, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1538, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1592, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2189, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1912, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1652, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2690, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1291, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1059, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1471, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1417, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1670, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2034, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1782, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2583, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1861, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2093, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1710, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4408, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2366, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1418, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1616, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3627, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0913, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0605, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1582, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1882, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.5596, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.20657 | Train accuracy: 94.44%\n","Test loss: 1.05036 | Test accuracy: 72.01%\n","\n","Epoch: 26\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1274, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5613, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3700, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0645, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4131, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2774, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4122, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1614, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2106, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2926, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2781, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2386, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2707, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2788, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5016, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2966, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1791, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3208, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3242, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.4266, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1690, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3047, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1277, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1251, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5780, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2232, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1945, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2181, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5516, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.2662, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.29212 | Train accuracy: 90.40%\n","Test loss: 0.82996 | Test accuracy: 75.87%\n","\n","Epoch: 27\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2632, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3918, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2796, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2370, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1494, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1425, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1627, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3087, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.5859, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1483, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1662, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2243, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1270, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1475, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2270, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1592, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2551, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2733, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2537, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1919, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0842, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1310, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1172, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2693, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1687, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.3739, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1726, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1523, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1267, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.2767, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.21888 | Train accuracy: 94.15%\n","Test loss: 0.79836 | Test accuracy: 77.47%\n","\n","Epoch: 28\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1536, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0941, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1109, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1019, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1323, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2809, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0581, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1800, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1147, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0704, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0988, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1012, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0786, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1395, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1657, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1307, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0554, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1022, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1401, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0726, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1039, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1082, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1727, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0176, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0946, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1891, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.2156, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0751, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1404, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.1781, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.12256 | Train accuracy: 96.55%\n","Test loss: 0.88841 | Test accuracy: 76.51%\n","\n","Epoch: 29\n","---------\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0729, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0631, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1193, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0821, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1185, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0932, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0614, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1720, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0677, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0575, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1642, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0588, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0542, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0372, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1105, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0886, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1198, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0432, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0479, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1316, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0293, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1660, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1545, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0618, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0724, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0766, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0595, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.0904, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 768]) torch.Size([32])\n","tensor(0.1065, grad_fn=<NllLossBackward0>) torch.Size([32]) torch.Size([32, 16])\n","torch.Size([15, 3, 224, 224]) torch.Size([15, 768]) torch.Size([15])\n","tensor(0.0665, grad_fn=<NllLossBackward0>) torch.Size([15]) torch.Size([15, 16])\n","Train loss: 0.08824 | Train accuracy: 97.50%\n","Test loss: 0.99405 | Test accuracy: 76.01%\n","\n"]}],"source":["torch.manual_seed(42)\n","\n","epochs = 30\n","for epoch in range(epochs):\n","    print(f\"Epoch: {epoch}\\n---------\")\n","    train_step(data_loader=train_loader,\n","        model=multimodal,\n","        loss_fn=loss_fn,\n","        optimizer=optimizer,\n","        accuracy_fn=accuracy_fn\n","    )\n","    test_step(data_loader=test_loader,\n","        model=multimodal,\n","        loss_fn=loss_fn,\n","        accuracy_fn=accuracy_fn\n","    )\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["QSlKQ7bo5lAy"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d2d5a20ecd96444cbdd51e467427eb5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bdeb63e5e034d3183850a9bb3fd1a8d","IPY_MODEL_ae1e3aef51384973ad65ff83778b0431","IPY_MODEL_20458aea57474325a397ad2ef4417b33"],"layout":"IPY_MODEL_50a3fcf7824f499da72067ab384337de"}},"8bdeb63e5e034d3183850a9bb3fd1a8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a541f9de0774eabbdc55abfcd145ff5","placeholder":"​","style":"IPY_MODEL_b74c23e378214db9977a3e29e5734685","value":"modules.json: 100%"}},"ae1e3aef51384973ad65ff83778b0431":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f76c850bb6b4456b343b0842aa7560a","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7b121821779430cbb3009202823897a","value":349}},"20458aea57474325a397ad2ef4417b33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d14f470cd96545819c473bdbe047e69e","placeholder":"​","style":"IPY_MODEL_934483888dd24eea9c6c2ac8efece25f","value":" 349/349 [00:00&lt;00:00, 23.7kB/s]"}},"50a3fcf7824f499da72067ab384337de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a541f9de0774eabbdc55abfcd145ff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b74c23e378214db9977a3e29e5734685":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f76c850bb6b4456b343b0842aa7560a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7b121821779430cbb3009202823897a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d14f470cd96545819c473bdbe047e69e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"934483888dd24eea9c6c2ac8efece25f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58603e9f5f5845dfba2ac318a6cae79d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65453e380ae9437ab79ed568b5038b8a","IPY_MODEL_5770ebda679942f8aaedfbe93d1c2cef","IPY_MODEL_601f83a08fe041c3be23403e9d4af97e"],"layout":"IPY_MODEL_753279af87334d27a4537d35259e999c"}},"65453e380ae9437ab79ed568b5038b8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e194d18077bf48f09f64bed12aa12eda","placeholder":"​","style":"IPY_MODEL_7e8a0cd2b13a4d32be15424a195005a9","value":"config_sentence_transformers.json: 100%"}},"5770ebda679942f8aaedfbe93d1c2cef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f00b2874750b44609e26e2e8888a8e1d","max":122,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6c035b4a50c49d69b5b230e0113ec4c","value":122}},"601f83a08fe041c3be23403e9d4af97e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acc6005120864598b2ecdf7c224b1012","placeholder":"​","style":"IPY_MODEL_b70e1353f8fc4b42ad3dc468f1b85c14","value":" 122/122 [00:00&lt;00:00, 3.85kB/s]"}},"753279af87334d27a4537d35259e999c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e194d18077bf48f09f64bed12aa12eda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e8a0cd2b13a4d32be15424a195005a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f00b2874750b44609e26e2e8888a8e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6c035b4a50c49d69b5b230e0113ec4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acc6005120864598b2ecdf7c224b1012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b70e1353f8fc4b42ad3dc468f1b85c14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e322bef520b342cab52efe6005e1ff60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ba254b4f4d547b79784482e3046bb80","IPY_MODEL_5a69d0a80dd648f9adabfdabf002164c","IPY_MODEL_538e252ce2ba4c58886eb2a5ebbac01a"],"layout":"IPY_MODEL_c8e57c101ae0486aab77f3ddd7ccf7f6"}},"8ba254b4f4d547b79784482e3046bb80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a47e8a22cb2641cebc2c5a6f9395559b","placeholder":"​","style":"IPY_MODEL_b365806041de4e9183fa9508277d412e","value":"README.md: 100%"}},"5a69d0a80dd648f9adabfdabf002164c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf91bd8c5dea492c8ff9753399d118d4","max":1886,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4154870c72a435bb0c5223911a97dbb","value":1886}},"538e252ce2ba4c58886eb2a5ebbac01a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b83483eac964603a5b9f016b3ff3814","placeholder":"​","style":"IPY_MODEL_368c91f4a18a4335a871e59a7221fa3c","value":" 1.89k/1.89k [00:00&lt;00:00, 43.0kB/s]"}},"c8e57c101ae0486aab77f3ddd7ccf7f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a47e8a22cb2641cebc2c5a6f9395559b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b365806041de4e9183fa9508277d412e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf91bd8c5dea492c8ff9753399d118d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4154870c72a435bb0c5223911a97dbb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b83483eac964603a5b9f016b3ff3814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"368c91f4a18a4335a871e59a7221fa3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37376a97cfad4f0b8bea034317763910":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb2ce4eb2dc240e1af5c863623a2db51","IPY_MODEL_33502b74c85243518773408fb92ca2fb","IPY_MODEL_f922cedae89d4df58a294cdb95b1a2fa"],"layout":"IPY_MODEL_e9210da4c54d4dc6a461453f14a98370"}},"fb2ce4eb2dc240e1af5c863623a2db51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_406cd0c77f3c4a909aee38ddf48b157d","placeholder":"​","style":"IPY_MODEL_ff72018a7e60406dad78c79b6da61ef7","value":"sentence_bert_config.json: 100%"}},"33502b74c85243518773408fb92ca2fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24ad674dd1d74903a681c9c3345afbb1","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13eb07ddfc904bb2a711991edfb6f9cb","value":53}},"f922cedae89d4df58a294cdb95b1a2fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee59ad0c5b014add866eb43e6a150f6c","placeholder":"​","style":"IPY_MODEL_8ac043b94e96435aacf6e2c552e83e47","value":" 53.0/53.0 [00:00&lt;00:00, 2.41kB/s]"}},"e9210da4c54d4dc6a461453f14a98370":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"406cd0c77f3c4a909aee38ddf48b157d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff72018a7e60406dad78c79b6da61ef7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24ad674dd1d74903a681c9c3345afbb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13eb07ddfc904bb2a711991edfb6f9cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee59ad0c5b014add866eb43e6a150f6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ac043b94e96435aacf6e2c552e83e47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07604382e84e4e288689bb587424b551":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd04c2f452dc4669a74d0d206fc97ac0","IPY_MODEL_5475a7583f384a2892a9c775123f8e2d","IPY_MODEL_b88533021cdb421eac692704e66b54bd"],"layout":"IPY_MODEL_60872dcbb5c749d09496669a5c192fe6"}},"dd04c2f452dc4669a74d0d206fc97ac0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fd4e3e657c547f18a44077b19fb4f1a","placeholder":"​","style":"IPY_MODEL_048900ed59f64799a029c0a32cc9dd06","value":"config.json: 100%"}},"5475a7583f384a2892a9c775123f8e2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb6f94dcad844c1f88f2a417947777db","max":804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c41fb20eb1c44417aaa01158e02f03c1","value":804}},"b88533021cdb421eac692704e66b54bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccf8bc1082ed419fbe846edf200d04fe","placeholder":"​","style":"IPY_MODEL_25a01e32bacd49dd8988be06b476120b","value":" 804/804 [00:00&lt;00:00, 13.5kB/s]"}},"60872dcbb5c749d09496669a5c192fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fd4e3e657c547f18a44077b19fb4f1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"048900ed59f64799a029c0a32cc9dd06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb6f94dcad844c1f88f2a417947777db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c41fb20eb1c44417aaa01158e02f03c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ccf8bc1082ed419fbe846edf200d04fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a01e32bacd49dd8988be06b476120b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b1de8510c424a7caed91f78f887e119":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7d8b029ad944ef49017682774b2085d","IPY_MODEL_60b6e03163544762b382db0f4ee200d1","IPY_MODEL_5918bce8f41c4b919a241fd3e01f98fb"],"layout":"IPY_MODEL_e6644e8416c3490ea4585edc5394ab2d"}},"a7d8b029ad944ef49017682774b2085d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87567b4ab0d9461ab043f05d180da961","placeholder":"​","style":"IPY_MODEL_de86f64545e1428281605e3c2a91aa48","value":"pytorch_model.bin: 100%"}},"60b6e03163544762b382db0f4ee200d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_112d74b8db6944b49f78c777316bbbba","max":1888170979,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7aa45a5bff14baab9634b46edc59582","value":1888170979}},"5918bce8f41c4b919a241fd3e01f98fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ae2afb2ccaa41c89966c9880d230735","placeholder":"​","style":"IPY_MODEL_4405a044b7f847769b38bd7c14f95aa9","value":" 1.89G/1.89G [00:39&lt;00:00, 52.3MB/s]"}},"e6644e8416c3490ea4585edc5394ab2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87567b4ab0d9461ab043f05d180da961":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de86f64545e1428281605e3c2a91aa48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"112d74b8db6944b49f78c777316bbbba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7aa45a5bff14baab9634b46edc59582":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ae2afb2ccaa41c89966c9880d230735":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4405a044b7f847769b38bd7c14f95aa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7e86bbd33764242a8b5d8bb059a1fa9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8a5adadcd704e389316276a739c80a2","IPY_MODEL_14238288a11b4e67b15c55aee4fcfcda","IPY_MODEL_3252e7b52987426e86023683341c1ba3"],"layout":"IPY_MODEL_321396c4e1ad418fb8e9239a0f877420"}},"c8a5adadcd704e389316276a739c80a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a81108fccefa4cbc808dd3f2c4c5c6d8","placeholder":"​","style":"IPY_MODEL_178fe89a113f4126b5367af73899f1a2","value":"tokenizer_config.json: 100%"}},"14238288a11b4e67b15c55aee4fcfcda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cedcd5bbd614f58b506c995f0c688c2","max":411,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2cea04f3cce0457da2a57ac8097a3c5a","value":411}},"3252e7b52987426e86023683341c1ba3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76e47fbdf99649848e4dd639bcecf30f","placeholder":"​","style":"IPY_MODEL_42748b121ee146e3b3f9ebaed1770887","value":" 411/411 [00:00&lt;00:00, 23.9kB/s]"}},"321396c4e1ad418fb8e9239a0f877420":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a81108fccefa4cbc808dd3f2c4c5c6d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178fe89a113f4126b5367af73899f1a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cedcd5bbd614f58b506c995f0c688c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cea04f3cce0457da2a57ac8097a3c5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76e47fbdf99649848e4dd639bcecf30f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42748b121ee146e3b3f9ebaed1770887":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e605076008394530aa6a8a187b121bdf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_accd738877134610b5c8c2c7b07a975e","IPY_MODEL_90959cfb066543679518e8af3ca9dbf7","IPY_MODEL_92940d4511c84a7384e832d23322ebd4"],"layout":"IPY_MODEL_e1722be15c8745588bd65b09e837552d"}},"accd738877134610b5c8c2c7b07a975e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5332efeb6de64994b6977f63517e9bb6","placeholder":"​","style":"IPY_MODEL_34c6e06c39bf4e79858b9a1408b5999f","value":"vocab.txt: 100%"}},"90959cfb066543679518e8af3ca9dbf7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac0f70c56da747dd902a0a1e8d9e6914","max":5220781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_248b6c0d2fae4774813d926ce0e00d35","value":5220781}},"92940d4511c84a7384e832d23322ebd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982ab752617545b197b32001b3676629","placeholder":"​","style":"IPY_MODEL_e795a2719dcf49afa32fb33d718d65d4","value":" 5.22M/5.22M [00:00&lt;00:00, 13.4MB/s]"}},"e1722be15c8745588bd65b09e837552d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5332efeb6de64994b6977f63517e9bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34c6e06c39bf4e79858b9a1408b5999f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac0f70c56da747dd902a0a1e8d9e6914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"248b6c0d2fae4774813d926ce0e00d35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"982ab752617545b197b32001b3676629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e795a2719dcf49afa32fb33d718d65d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abfb9cb3931a4352b2a5112b371dd06d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b6414efeb874a2f9186601a1b7b7349","IPY_MODEL_33a920eea663491f8f3fec25da6bd749","IPY_MODEL_736c7783cb0845f7b2a84ab870cfd41c"],"layout":"IPY_MODEL_894b491cb3cc4a8ab0bf4d895dd5b271"}},"7b6414efeb874a2f9186601a1b7b7349":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bfa8371f8dd4b07a1cb02af4b6b3082","placeholder":"​","style":"IPY_MODEL_600099c19a824a2092be84e726fc0b63","value":"tokenizer.json: 100%"}},"33a920eea663491f8f3fec25da6bd749":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_726135d999f74ec49e8990a06e0ef970","max":9621556,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dffacd93f675470e9734fa6ee60222f4","value":9621556}},"736c7783cb0845f7b2a84ab870cfd41c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d93a4fd4d2c4244b0db562a74a4c5f0","placeholder":"​","style":"IPY_MODEL_49799bf49a1548dfb2d7ca1661e9eca5","value":" 9.62M/9.62M [00:00&lt;00:00, 19.7MB/s]"}},"894b491cb3cc4a8ab0bf4d895dd5b271":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bfa8371f8dd4b07a1cb02af4b6b3082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"600099c19a824a2092be84e726fc0b63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"726135d999f74ec49e8990a06e0ef970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dffacd93f675470e9734fa6ee60222f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d93a4fd4d2c4244b0db562a74a4c5f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49799bf49a1548dfb2d7ca1661e9eca5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfd69952aa9f48358fcdb99490595187":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f82dc95ed7446e49dcd5a06ce6e436d","IPY_MODEL_c057ccc84087452497dc56c1eaa93627","IPY_MODEL_b4e1cf8299064cdd8cab8672dd6e31b0"],"layout":"IPY_MODEL_a0c8e3e0a7aa404fbdbea51cf28cf655"}},"1f82dc95ed7446e49dcd5a06ce6e436d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5d5cd10ae7e458b8ca4b9e019dd0790","placeholder":"​","style":"IPY_MODEL_2a06cb3b687e4142b5779b96354ecce0","value":"special_tokens_map.json: 100%"}},"c057ccc84087452497dc56c1eaa93627":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d6cbd5173d14b0e80fd7a927b2c8d80","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a322ff398e3e4b008e68360dda8542c0","value":112}},"b4e1cf8299064cdd8cab8672dd6e31b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d46ff0bd8af3440ea8b929be66fb2ea0","placeholder":"​","style":"IPY_MODEL_ebafa061fc7a4d20b9dcc2a521b5d620","value":" 112/112 [00:00&lt;00:00, 6.29kB/s]"}},"a0c8e3e0a7aa404fbdbea51cf28cf655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5d5cd10ae7e458b8ca4b9e019dd0790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a06cb3b687e4142b5779b96354ecce0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d6cbd5173d14b0e80fd7a927b2c8d80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a322ff398e3e4b008e68360dda8542c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d46ff0bd8af3440ea8b929be66fb2ea0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebafa061fc7a4d20b9dcc2a521b5d620":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aebc0b6e56e64a268414cd3253b802b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21572f8f2c114e4ea4f3f11fa62ace23","IPY_MODEL_675d52bb107849b186ddc604a8fb9158","IPY_MODEL_55a1294713444227af596fa400b76425"],"layout":"IPY_MODEL_40be60d8de33473ba74176ad942b5a51"}},"21572f8f2c114e4ea4f3f11fa62ace23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70fbf87d0a654c2796c23a1f53a3dd93","placeholder":"​","style":"IPY_MODEL_7c06d0d1e2734b83b15593cb0be7600c","value":"1_Pooling/config.json: 100%"}},"675d52bb107849b186ddc604a8fb9158":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3af87eb99e854724a41e0c44d2f7bddb","max":191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4b5cd0237b0459eabdb8c0a1298c053","value":191}},"55a1294713444227af596fa400b76425":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_114b7eeae87c4bad8fe2b7fcedad4a5a","placeholder":"​","style":"IPY_MODEL_6352c68002b14cf895fb2c5097adba49","value":" 191/191 [00:00&lt;00:00, 8.30kB/s]"}},"40be60d8de33473ba74176ad942b5a51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70fbf87d0a654c2796c23a1f53a3dd93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c06d0d1e2734b83b15593cb0be7600c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3af87eb99e854724a41e0c44d2f7bddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b5cd0237b0459eabdb8c0a1298c053":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"114b7eeae87c4bad8fe2b7fcedad4a5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6352c68002b14cf895fb2c5097adba49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}